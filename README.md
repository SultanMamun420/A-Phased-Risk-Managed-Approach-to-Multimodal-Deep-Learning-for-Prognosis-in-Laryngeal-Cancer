# A-Phased-Risk-Managed-Approach-to-Multimodal-Deep-Learning-for-Prognosis-in-Laryngeal-Cancer
A Phased, Risk-Managed Approach to Multimodal Deep Learning for Prognosis in Laryngeal Cancer Using MRI-Guided Radiotherapy and Clinical Data
A Phased, Risk-Managed Approach to Multimodal Deep Learning for Prognosis in Laryngeal Cancer Using MRI-Guided Radiotherapy and Clinical Data

Authors: Sultan Mamun¹,  Prof. Dr. Rytis Maskeliūnas¹
¹School of Mechanical Engineering,  Yangzhou University, Jiangsu, China 
¹Faculty of Informatics, Kaunas University of Technology, Kaunas, Lithuania

Corresponding Author: 

Abstract:
Background: Integrating longitudinal MRI-guided radiotherapy (MRIgRT) with clinical data presents a complex multimodal challenge for improving prognostic accuracy in laryngeal cancer. This study proposes a novel, phased framework that sequentially de-risks model development, from synthetic data generation to the deployment of a Sparse Mixture-of-Experts (SparseMoE) architecture with inherent explainability.
Methods: We implemented a six-step framework: 1) Problem Definition, 2) Creation of a validated synthetic medical imaging dataset (n=1000 samples), 3) Building a Baseline Static Fusion model, 4) Building a SparseMoE model, 5) Establishing a training protocol, and 6) Model training and evaluation. Model performance was assessed via accuracy, F1-score, and confusion matrices, with expert gating behavior analyzed for explainability.
Results: The Baseline model achieved superior final performance (Test Accuracy: 99.44%, F1: N/A) compared to the SparseMoE model (Test Accuracy: 97.00%, F1: 0.9698). However, the SparseMoE offered significant parameter efficiency (176,741 total vs. 228,099 in Baseline) and dynamic, interpretable routing, with experts selected equally (50%/50%). The Baseline model showed faster convergence and lower final loss.
Conclusion: This work demonstrates a practical, risk-managed pathway for developing complex multimodal prognostic models. While the Baseline model achieved higher accuracy on this synthetic dataset, the SparseMoE provides a scalable, interpretable alternative crucial for clinical translation. Future work will integrate temporal components, causal regularization, and validation on real-world MRIgRT data.

Keywords: Multimodal Deep Learning, Laryngeal Cancer Prognosis, MRI-Guided Radiotherapy (MRIgRT), Sparse Mixture-of-Experts, Synthetic Medical Data, Explainable AI.

1. Introduction
Laryngeal cancer prognosis remains challenging due to the complex interplay of tumor morphology, treatment response captured via longitudinal imaging, and heterogeneous clinical factors [1]. The advent of MRI-guided radiotherapy (MRIgRT) offers unprecedented soft-tissue contrast for tracking anatomical changes during treatment, presenting a rich multimodal data fusion problem [2]. While deep learning holds promise for integrating imaging and clinical data, its application in oncology is hampered by data scarcity, model opacity, and the high risk of failure in complex pipelines [3, 4].

This study introduces a phased, risk-managed methodological framework designed to systematically address these barriers. We advocate for a development cycle that begins with synthetic data to validate architectures, proceeds through controlled comparisons of fusion strategies—contrasting a static Baseline with a dynamic, interpretable Sparse Mixture-of-Experts (SparseMoE) model—and culminates in rigorous explainability analysis [5]. This stepwise approach mitigates project risk by ensuring foundational components are sound before committing to clinical data. Our contributions are: (i) a reproducible, six-phase development framework for multimodal oncology AI; (ii) a comparative analysis of a Static Fusion Baseline versus a parameter-efficient SparseMoE on a synthetic multimodal laryngeal cancer dataset; and (iii) an in-depth examination of gating behavior, providing a blueprint for model interpretability in clinical decision-support systems.

2. Materials and Methods
2.1. Phased Development Framework
Our framework comprises nine distinct, sequential steps to de-risk development: Step 1: Problem Formulation; Step 2: Synthetic Data Creation; Step 3: Baseline Model (Static Fusion) Development; Step 4: SparseMoE Model Development; Step 5: Training Framework Configuration; Step 6: Model Training; Step 7: Performance Evaluation; Step 8: Explainability Analysis (Gating Behavior); Step 9: Performance Comparison and Ablation.

2.2. Synthetic Multimodal Dataset
To circumvent initial data limitations and validate our pipeline, we generated a synthetic medical imaging dataset simulating fused MRIgRT and clinical feature vectors, a technique supported for architectural validation [6]. The dataset comprised 700 training, 100 validation, and 200 test samples, simulating the prognostic classification task for laryngeal cancer.



Fig. 01. Synthetic Medical image (Simulating)

2.3. Model Architectures
2.3.1. Baseline Model (Static Fusion): This model served as our control, employing a standard late-fusion paradigm where features from imaging and clinical data pathways are concatenated before a final classification head [7]. It contained 228,099 parameters.

2.3.2. Sparse Mixture-of-Experts (SparseMoE) Model: Inspired by models like GShard and Switch Transformers [8, 9], our Sparse MoE model uses a trainable gating network to dynamically route each input sample to a subset of specialized expert networks (two experts in this implementation). This promotes efficient, conditional computation. The total model had 176,741 parameters, with only 31,333 being trainable gating parameters, showcasing high parameter efficiency.

2.4. Training Protocol
Both models were trained for 30 epochs using a fixed learning rate of 1e-4, with cross-entropy loss. Training was monitored using training/validation loss and accuracy.

3. Results
3.1. Model Training Dynamics
The Baseline model demonstrated rapid convergence, achieving 99.29% training accuracy and 98.00% validation accuracy by epoch 30, with corresponding losses dropping to 0.0329 and 0.0408.



Fig. 02. Baline Model- training and Accuracy Curves

In contrast, the SparseMoE model learned more slowly, finalizing at 87.00% training accuracy and 83.00% validation accuracy, with higher final losses (0.4088 and 0.5235). 



Fig. 03. Sparse MoE Model- training and Accuracy Curves

This suggests the Baseline model more effectively learned the synthetic dataset's patterns, while the MoE's dynamic routing presented a more challenging optimization landscape.

3.2. Final Performance and Confusion Analysis
On the held-out test set, the Baseline model achieved an accuracy of 99.44%.



Fig. 04. Confusion Matrix- Baseline Model

 The Sparse MoE model attained a test accuracy of 97.00%, with an F1-score of 0.9698 and an average prediction confidence of 0.8258.  



Fig. 05. Confusion Matrix- Sparse MoE Model

Despite lower accuracy, the Sparse MoE's performance remains high, and its confusion matrix reveals a balanced error profile.

3.3. Explainability: Gating Network Analysis
A critical advantage of the SparseMoE is its inherent interpretability. Analysis of the gating network on the test set (n=400 routing decisions) revealed a perfectly balanced expert utilization: Expert 1 and Expert 2 were each selected for 200 samples (50.0%) (Page 4, Fig. 06). 


Fig. 06. Gating Weights and Expert selection Distribution

This indicates the gating network learned to leverage both specialists equally for the synthetic task, providing a transparent view into model decision-making—a feature absent in the static Baseline [10].

3.4. Comparative Analysis
The performance comparison consolidates these findings. 



Fig. 07. Model Performance Comparison  and Validation Accuracy Over time

The Baseline model achieved higher peak accuracy faster. However, the SparseMoE offers a compelling trade-off: a slight reduction in accuracy for significant gains in parameter efficiency and, most importantly, dynamic explainability via gating behavior, which is essential for clinical trust and model debugging [11].

4. Discussion
Our phased framework successfully facilitated the controlled development and comparison of two multimodal deep learning architectures. The superior accuracy of the static Baseline model aligns with expectations on a well-defined synthetic task where feature relationships are consistent. However, the SparseMoE's balanced expert utilization and efficient structure demonstrate its potential for real-world scenarios characterized by data heterogeneity and a need for model transparency [12].

The observed optimization difficulty in the SparseMoE is a known challenge in routing networks [9]. Future iterations will benefit from more advanced gating initialization and regularization techniques. The primary limitation of this study is the use of synthetic data. While invaluable for de-risking, it does not capture the full noise and complexity of real MRIgRT and clinical data.

5. Conclusion and Future Directions
We present a validated, phased approach for developing multimodal prognostic models in oncology, demonstrating its utility through a comparative study of fusion techniques. The framework lowers the barrier to entry for complex AI projects by ensuring methodological soundness before clinical data integration.

Immediate future work will focus on:

1.Adding a temporal component to model longitudinal MRIgRT data [13].

2.Implementing causal regularization (CausalReg) to improve model robustness and generalizability [14].

3.Integrating Bayesian uncertainty quantification for confidence estimation [15].

4.Developing a clinician-in-the-loop evaluation protocol for human-AI collaboration.

5.Validating and refining the models on a real-world MRIgRT laryngeal cancer dataset, the crucial next step for clinical translation.

6.  Acknowledgments
This research is supported by a state-funded doctoral position at Kaunas University of Technology. Data access was provided by LSMU Kauno Klinikos under established collaboration agreements.

7. Code and Data Availability
All code, preprocessing pipelines, and derived feature datasets will be made publicly available on “https://colab.research.google.com/drive/1UXxfg5jH7vm1p1iokkcEXeQMpNcJDi6l”. Raw patient data cannot be shared due to ethical restrictions.

8.References

[1]Patel, S. G., & Shah, J. P. (2005). TNM staging of cancers of the head and neck: striving for uniformity among diversity. CA: A Cancer Journal for Clinicians, 55(4), 242-258. 

[2]Mutic, S., & Dempsey, J. F. (2014). The ViewRay system: magnetic resonance-guided and controlled radiotherapy. Seminars in Radiation Oncology, 24(3), 196-199. 

[3] Esteva, A., Robicquet, A., Ramsundar, B., Kuleshov, V., DePristo, M., Chou, K., ... & Dean, J. (2019). A guide to deep learning in healthcare. Nature Medicine, 25(1), 24-29. 

[4] Litjens, G., Kooi, T., Bejnordi, B. E., Setio, A. A. A., Ciompi, F., Ghafoorian, M., ... & van der Laak, J. A. (2017). A survey on deep learning in medical image analysis. Medical Image Analysis, 42, 60-88. 

[5]Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5), 206-215. 

[6] Shin, H. C., Tenenholtz, N. A., Rogers, J. K., Schwarz, C. G., Senjem, M. L., Gunter, J. L., ... & Michalski, M. H. (2018). Medical image synthesis for data augmentation and anonymization using generative adversarial networks. In Simulation and Synthesis in Medical Imaging (pp. 1-11). Springer, Cham.

[7] Huang, S. C., Pareek, A., Seyyedi, S., Banerjee, I., & Lungren, M. P. (2020). Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines. NPJ Digital Medicine, 3(1), 1-9. 

[8] Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., ... & Chen, Z. (2021). Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668.

[9] Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120), 1-39. 

[10] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., & Dean, J. (2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538.

[11] Tonekaboni, S., Joshi, S., McCradden, M. D., & Goldenberg, A. (2019). What clinicians want: contextualizing explainable machine learning for clinical end use. Proceedings of the 4th Machine Learning for Healthcare Conference.

[12] Ravi, D., Wong, C., Deligianni, F., Berthelot, M., Andreu-Perez, J., Lo, B., & Yang, G. Z. (2017). Deep learning for health informatics. IEEE Journal of Biomedical and Health Informatics, 21(1), 4-21. 

[13] Jing, B., Xie, P., & Xing, E. (2018). On the automatic generation of medical imaging reports. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics.

[14] Schölkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalchbrenner, N., Goyal, A., & Bengio, Y. (2021). Toward causal representation learning. Proceedings of the IEEE, 109(5), 612-634. 

[15] Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning (pp. 1050-1059). PMLR.

Figure Captions:

Fig. 01: Synthetic Medical image (Simulating) used for initial pipeline validation.

Fig. 02: Baseline Model - Training and Validation Loss/Accuracy Curves.

Fig. 03: Sparse MoE Model - Training and Validation Loss/Accuracy Curves.

Fig. 04: Confusion Matrix for the Baseline Model on the test set (Accuracy: 0.9944).

Fig. 05: Confusion Matrix for the Sparse MoE Model on the test set (Accuracy: 0.9700, F1-Score: 0.9698).

Fig. 06: Gating Weights and Expert Selection Distribution for the SparseMoE model, showing balanced expert utilization (50%/50%).

Fig. 07: Model Performance Comparison summarizing validation accuracy over training time and final metrics.
